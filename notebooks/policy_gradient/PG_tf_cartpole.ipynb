{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "### Introduction\n",
    "On this notebook we will train a policy network to solve a particular problem on the OpenAI Environment. We will train this policy network with the REINFORCE algorithm.\n",
    "![alt text](imgs/policy_stochastic.png \"Game\")\n",
    "\n",
    "#### REINFORCE Algorithm\n",
    "#### Short Introduction\n",
    "The REINFORCE algorithm is one implementation of the Policy Gradient family of algorithms, the idea of the algorithm is to have the policy represented by a neural network with $\\theta$ parameters. The main idea of the algorithm is to change the network parameters to make the actions that provided positive rewards to be more probable to happen. The training phase optimize the following loss function:\n",
    "$$\\mathcal{L}=-\\hat{Q}(s,a).log\\pi_{\\theta}(a|s)$$\n",
    "Where:\n",
    "* $\\hat{Q}(s,a)$: It's an estimate of the state action function, that will modulate the probability of good actions to happen more often.\n",
    "* $\\pi_{\\theta}(a|s)$: It's the neural network that represent the policy and returns an distribution of actions given an state.\n",
    "\n",
    "The steps of the algorithm\n",
    "1. Initialize Network at random\n",
    "2. Play N episodes saving their transitions \"k\" (s,a,r,s')\n",
    "3. For every episode calculate $\\hat{Q}(s,a)=\\sum_{i=0} \\gamma^i r_i$\n",
    "4. Perform SGD to minimize the loss: $\\mathcal{L}=-\\hat{Q}(s,a).log\\pi_{\\theta}(a|s)$\n",
    "5. Repeat 2 until convergence \n",
    "\n",
    "#### Disadvantages of Policy Gradients\n",
    "* Full episodes are required, we need to wait a full episode to complete\n",
    "* High Gradients Variance: This issue can be handled by subtracting a baseline from the Value estimation\n",
    "* Exploration: The agent can converge to a local-optimal area and won't explore efficiently anymore. This can be solved by the Entropy Bonus technique that basically subtract the entropy of the policy from the loss function.\n",
    "* Correlation between samples: This can be remedy by using parallel environments with same policy and using the experiences from different environments to train the policy.\n",
    "* Less sample efficient: To deal with this we need another algorithm (Actor-Critic)\n",
    "\n",
    "The image bellow can highlight the full episodic issue, where we have 2 trajectories where one or more actions could be bad, but as the final total score is good those bad actions will be averaged. That's one of the reasons why Policy Gradient methods are less sample efficient.\n",
    "![alt text](imgs/episode_problem.png \"Game\")\n",
    "\n",
    "#### Cartpole Rules\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "#### References\n",
    "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
    "* http://karpathy.github.io/2016/05/31/rl/\n",
    "* https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf\n",
    "* https://leimao.github.io/article/REINFORCE-Policy-Gradient/\n",
    "* https://towardsdatascience.com/an-intuitive-explanation-of-policy-gradient-part-1-reinforce-aa4392cbfd3c\n",
    "* https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f\n",
    "* https://gist.github.com/tamlyn/a9d2b3990f9dab0f82d1dfc1588c876a\n",
    "* http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "* https://www.youtube.com/watch?v=iOh7QUZGyiU&list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs\n",
    "* https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html\n",
    "* https://medium.com/@gabogarza/deep-reinforcement-learning-policy-gradients-8f6df70404e6\n",
    "* https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4\n",
    "* http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf\n",
    "* https://fosterelli.co/entropy-loss-for-reinforcement-learning\n",
    "* https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment and Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of rewards consider as win: 475.0\n",
      "Action space: 2\n",
      "Observation space: (4,)\n",
      "Reward range: (-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# Uncomment for this environment (takes longer to train)\n",
    "#env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "entropy_beta = 0.01\n",
    "gamma = 0.99\n",
    "num_episodes=1500\n",
    "\n",
    "# Show some information from the environment\n",
    "print('Amount of rewards consider as win:', env.spec.reward_threshold)\n",
    "print('Action space:', env.action_space.n)\n",
    "print('Observation space:', env.observation_space.shape)\n",
    "print('Reward range:', env.reward_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Random Seeds\n",
    "To make the experiment reproducible, we will fix the following random seeds:\n",
    "* OpenAI Gym\n",
    "* Numpy\n",
    "* Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network\n",
    "Define the policy Neural Network, which inputs will be the state and output an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(Model):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        state_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        num_hidden = 128\n",
    "\n",
    "        self.FC1 = Dense(num_hidden, activation='relu', input_dim=state_space, use_bias=False)\n",
    "        self.DP1 = Dropout(rate=0.5)\n",
    "        self.FC2 = Dense(action_space, activation='softmax', use_bias=False)\n",
    "\n",
    "        # Overall reward and loss history (Just for plotting training information)\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Episode policy log probability(action) and reward history\n",
    "        self.episode_log_prob_actions = []\n",
    "        self.episode_log_prob_actions_dist = []\n",
    "        self.episode_prob_actions = []\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def call(self, x, keep_history=False):\n",
    "        # Convert state to tensor\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = np.expand_dims(x, axis=0)        \n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float64)          \n",
    "        x = self.FC1(x)\n",
    "        x = self.DP1(x)\n",
    "        action_probs = self.FC2(x)        \n",
    "        \n",
    "        # Execute model and sample it's outputs        \n",
    "        action = tf.squeeze(tf.random.categorical(action_probs,1), axis=-1)        \n",
    "        \n",
    "        #print('action_probs:', action_probs.size())\n",
    "        #print('distribution.log_prob:', distribution.log_prob(action).size())\n",
    "        #print('F.log_prob(action_probs):', torch.log(action_probs).size())\n",
    "        \n",
    "        # Concatenate log probability of the action to be used on the loss function\n",
    "        if keep_history:\n",
    "            print(action)\n",
    "            print(action.shape)\n",
    "            log_prob_action = tf.math.log(action_probs[0][action[0]])\n",
    "            self.episode_log_prob_actions = tf.concat([self.episode_log_prob_actions, \n",
    "                                                     log_prob_action], axis=0)\n",
    "            self.episode_log_prob_actions_dist = tf.concat([self.episode_log_prob_actions_dist, \n",
    "                                                     tf.math.log(action_probs)], axis=0)\n",
    "            self.episode_prob_actions = tf.concat([self.episode_prob_actions, action_probs], axis=0)\n",
    "        \n",
    "        return  tf.squeeze(action, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer= tf.keras.optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the update policy function\n",
    "This function is called after we gather a complete episode, and it does the following tasks:\n",
    "1. Calculate the discounted Rewards: $\\hat{Q}(s,a)=\\sum_{i=0} \\gamma^i r_i$\n",
    "2. Calculate the baseline (rewards mean) and normalize the discounted rewards\n",
    "3. Calculate the entropy loss (Optional)\n",
    "4. Calculate the Policy loss\n",
    "5. Calculate the complete loss\n",
    "6. Do the loss backpropagation (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_policy(use_baseline = False, use_entropy = False):\n",
    "    R = 0\n",
    "    rewards = []\n",
    "\n",
    "    # Discount future rewards starting from the most recent reward backwards\n",
    "    for r in reversed(policy.episode_rewards):\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Scale rewards to make the advantage be 1/-1 (Zero-mean Unit variance)\n",
    "    # This is one of multiple ways to implement the advantage, for example if you use another network\n",
    "    # to learn the advantage this will become the actor-critic algorithm.\n",
    "    #rewards = tf.to_float(rewards)\n",
    "    \n",
    "    if use_baseline:\n",
    "        baseline = tf.math.reduce_mean(rewards)\n",
    "        # Calculate the value estimation with a baseline (Without the baseline will be harder to converge)\n",
    "        Q_estim = (rewards - baseline) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    else:\n",
    "        Q_estim = rewards\n",
    "\n",
    "    # Calculate the entropy \n",
    "    entropy_loss = -entropy_beta * (tf.math.reduce_sum(\n",
    "        policy.episode_prob_actions * policy.episode_log_prob_actions_dist))\n",
    "    \n",
    "    # Calculate the policy loss\n",
    "    police_loss = tf.math.reduce_sum(-Q_estim * policy.episode_log_prob_actions)\n",
    "    \n",
    "    if use_entropy:\n",
    "        # Calculate loss log_prob(action) * rewards\n",
    "        loss = police_loss - entropy_loss    \n",
    "    else:\n",
    "        loss = police_loss\n",
    "\n",
    "    # Update network weights\n",
    "    #optimizer.zero_grad()\n",
    "    #loss.backward()\n",
    "    #optimizer.step()\n",
    "\n",
    "    # Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.episode_rewards))\n",
    "    policy.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check untrained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardoaraujo/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for time in range(50):\n",
    "    # Get an action from the Policy\n",
    "    action = policy(state)\n",
    "\n",
    "    # Render screen\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "(1,)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Ranks of all input tensors should match: shape[0] = [0] vs. shape[1] = [] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-245-149bf02c3b11>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, keep_history)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mlog_prob_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             self.episode_log_prob_actions = tf.concat([self.episode_log_prob_actions, \n\u001b[0;32m---> 46\u001b[0;31m                                                      log_prob_action], axis=0)\n\u001b[0m\u001b[1;32m     47\u001b[0m             self.episode_log_prob_actions_dist = tf.concat([self.episode_log_prob_actions_dist, \n\u001b[1;32m     48\u001b[0m                                                      tf.math.log(action_probs)], axis=0)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1269\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1270\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1199\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         return concat_v2_eager_fallback(\n\u001b[0;32m-> 1201\u001b[0;31m             values, axis, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   1202\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2_eager_fallback\u001b[0;34m(values, axis, name, ctx)\u001b[0m\n\u001b[1;32m   1247\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"N\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tidx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m   _result = _execute.execute(b\"ConcatV2\", 1, inputs=_inputs_flat,\n\u001b[0;32m-> 1249\u001b[0;31m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m   1250\u001b[0m   _execute.record_gradient(\n\u001b[1;32m   1251\u001b[0m       \"ConcatV2\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Ranks of all input tensors should match: shape[0] = [0] vs. shape[1] = [] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(1000):\n",
    "        # Get an action from the Policy and keep the historical data\n",
    "        action = policy(state, keep_history=True)\n",
    "\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Save reward\n",
    "        policy.episode_rewards.append(reward)\n",
    "        rewards_episode.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate score to determine when the environment has been solved\n",
    "    length_episode.append(time)\n",
    "    mean_length_episode = np.mean(length_episode[-100:])\n",
    "    mean_rewards_episode = np.mean(rewards_episode)\n",
    "    \n",
    "    # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "    update_policy()\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for time in range(1000):\n",
    "    action = policy(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving sample efficiency\n",
    "Policy Gradient algorithms are already sample-efficiency worse than value based algorithms. Calculating a baseline and subtracting from your value estimate $\\hat{Q}$ will improve sample efficiency. Here is a list of poossible choices for baselines:\n",
    "* Some constant value, for example the mean of the discounted rewards\n",
    "* Moving average of discounted rewards\n",
    "* Value of state V(s)\n",
    "Also depending if we use anohter network to learn out baseline we will be working on the Actor-Critic family o algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(1000):\n",
    "        # Get an action from the Policy and keep the historical data\n",
    "        action = policy(state, keep_history=True)\n",
    "\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Save reward\n",
    "        policy.episode_rewards.append(reward)\n",
    "        rewards_episode.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate score to determine when the environment has been solved\n",
    "    length_episode.append(time)\n",
    "    mean_length_episode = np.mean(length_episode[-100:])\n",
    "    mean_rewards_episode = np.mean(rewards_episode)\n",
    "    \n",
    "    # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "    update_policy(use_baseline=True)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Exploration/Exploitation\n",
    "In order to avoid the policy to conferge fast and stop exploring (Get stuck in local minima) in the environment, there is a trick called entropy bonus.\n",
    "$$H(\\pi)=-\\sum{\\pi_{\\theta}(a|s).log\\pi_{\\theta}(a|s)}$$\n",
    "This will simply show how much uncertain the policy is about it's actions. \n",
    "We just need to subtract the entropy bons from the loss function, forcing the policy to take more time to be certain and exploring more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(1000):\n",
    "        # Get an action from the Policy and keep the historical data\n",
    "        action = policy(state, keep_history=True)\n",
    "\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Save reward\n",
    "        policy.episode_rewards.append(reward)\n",
    "        rewards_episode.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate score to determine when the environment has been solved\n",
    "    length_episode.append(time)\n",
    "    mean_length_episode = np.mean(length_episode[-100:])\n",
    "    mean_rewards_episode = np.mean(rewards_episode)\n",
    "    \n",
    "    # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "    update_policy(use_baseline=True, use_entropy=True)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = policy(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
