{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "### Introduction\n",
    "On this notebook we will train a policy network to solve a particular problem on the OpenAI Environment. We will train this policy network with the REINFORCE algorithm.\n",
    "![alt text](imgs/policy_stochastic.png \"Game\")\n",
    "\n",
    "#### REINFORCE Algorithm\n",
    "#### Short Introduction\n",
    "The REINFORCE algorithm is one implementation of the Policy Gradient family of algorithms, the idea of the algorithm is to have the policy represented by a neural network with $\\theta$ parameters. The main idea of the algorithm is to change the network parameters to make the actions that provided positive rewards to be more probable to happen. The training phase optimize the following loss function:\n",
    "$$\\mathcal{L}=-\\hat{Q}(s,a).log\\pi_{\\theta}(a|s)$$\n",
    "Where:\n",
    "* $\\hat{Q}(s,a)$: It's an estimate of the state action function, that will modulate the probability of good actions to happen more often.\n",
    "* $\\pi_{\\theta}(a|s)$: It's the neural network that represent the policy and returns an distribution of actions given an state.\n",
    "\n",
    "The steps of the algorithm\n",
    "1. Initialize Network at random\n",
    "2. Play N episodes saving their transitions \"k\" (s,a,r,s')\n",
    "3. For every episode calculate $\\hat{Q}(s,a)=\\sum_{i=0} \\gamma^i r_i$\n",
    "4. Perform SGD to minimize the loss: $\\mathcal{L}=-\\hat{Q}(s,a).log\\pi_{\\theta}(a|s)$\n",
    "5. Repeat 2 until convergence \n",
    "\n",
    "#### Disadvantages of Policy Gradients\n",
    "* Full episodes are required, we need to wait a full episode to complete\n",
    "* High Gradients Variance: This issue can be handled by subtracting a baseline from the Value estimation\n",
    "* Exploration: The agent can converge to a local-optimal area and won't explore efficiently anymore. This can be solved by the Entropy Bonus technique that basically subtract the entropy of the policy from the loss function.\n",
    "* Correlation between samples: This can be remedy by using parallel environments with same policy and using the experiences from different environments to train the policy.\n",
    "* Less sample efficient: To deal with this we need another algorithm (Actor-Critic)\n",
    "\n",
    "The image bellow can highlight the full episodic issue, where we have 2 trajectories where one or more actions could be bad, but as the final total score is good those bad actions will be averaged. That's one of the reasons why Policy Gradient methods are less sample efficient.\n",
    "![alt text](imgs/episode_problem.png \"Game\")\n",
    "\n",
    "#### Cartpole Rules\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "#### References\n",
    "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
    "* http://karpathy.github.io/2016/05/31/rl/\n",
    "* https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf\n",
    "* https://leimao.github.io/article/REINFORCE-Policy-Gradient/\n",
    "* https://towardsdatascience.com/an-intuitive-explanation-of-policy-gradient-part-1-reinforce-aa4392cbfd3c\n",
    "* https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f\n",
    "* https://gist.github.com/tamlyn/a9d2b3990f9dab0f82d1dfc1588c876a\n",
    "* http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "* https://www.youtube.com/watch?v=iOh7QUZGyiU&list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs\n",
    "* https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html\n",
    "* https://medium.com/@gabogarza/deep-reinforcement-learning-policy-gradients-8f6df70404e6\n",
    "* https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4\n",
    "* http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf\n",
    "* https://fosterelli.co/entropy-loss-for-reinforcement-learning\n",
    "* https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment and Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of rewards consider as win: 475.0\n",
      "Action space: 2\n",
      "Observation space: (4,)\n",
      "Reward range: (-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# Uncomment for this environment (takes longer to train)\n",
    "#env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "entropy_beta = 0.01\n",
    "gamma = 0.99\n",
    "num_episodes=1500\n",
    "\n",
    "# Show some information from the environment\n",
    "print('Amount of rewards consider as win:', env.spec.reward_threshold)\n",
    "print('Action space:', env.action_space.n)\n",
    "print('Observation space:', env.observation_space.shape)\n",
    "print('Reward range:', env.reward_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Random Seeds\n",
    "To make the experiment reproducible, we will fix the following random seeds:\n",
    "* OpenAI Gym\n",
    "* Numpy\n",
    "* Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network\n",
    "Define the policy Neural Network, which inputs will be the state and output an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(Model):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        state_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        num_hidden = 128\n",
    "\n",
    "        self.FC1 = Dense(num_hidden, activation='relu', input_dim=state_space, use_bias=False)\n",
    "        self.DP1 = Dropout(rate=0.5)\n",
    "        self.FC2 = Dense(action_space, activation='softmax', use_bias=False)\n",
    "\n",
    "        # Overall reward and loss history (Just for plotting training information)\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Episode policy log probability(action) and reward history\n",
    "        self.episode_log_prob_actions = None\n",
    "        self.episode_log_prob_actions_dist = None\n",
    "        self.episode_prob_actions = None\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def call(self, x, keep_history=False):\n",
    "        # Convert state to tensor\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = np.expand_dims(x, axis=0)        \n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float64)          \n",
    "        x = self.FC1(x)\n",
    "        x = self.DP1(x)\n",
    "        action_probs = self.FC2(x)        \n",
    "        \n",
    "        # Execute model and sample it's outputs        \n",
    "        action = tf.squeeze(tf.random.categorical(action_probs,1), axis=-1)        \n",
    "        \n",
    "        #print('action_probs:', action_probs.size())\n",
    "        #print('distribution.log_prob:', distribution.log_prob(action).size())\n",
    "        #print('F.log_prob(action_probs):', torch.log(action_probs).size())\n",
    "        \n",
    "        # Concatenate log probability of the action to be used on the loss function\n",
    "        if keep_history:                        \n",
    "            # Concatenate the log of the choosen action\n",
    "            if self.episode_log_prob_actions is None:                \n",
    "                self.episode_log_prob_actions = [tf.math.log(action_probs[0][action[0]])]                               \n",
    "            else:                \n",
    "                self.episode_log_prob_actions = tf.concat([self.episode_log_prob_actions, \n",
    "                                                           [tf.math.log(action_probs[0][action[0]])]], axis=0)\n",
    "            \n",
    "            # Concatenate the log the action distribution\n",
    "            if self.episode_log_prob_actions_dist is None:\n",
    "                self.episode_log_prob_actions_dist = tf.math.log(action_probs)\n",
    "            else:\n",
    "                self.episode_log_prob_actions_dist = tf.concat([self.episode_log_prob_actions_dist, \n",
    "                                                                tf.math.log(action_probs)], axis=0)\n",
    "            # Concatenate the action distribution\n",
    "            if self.episode_prob_actions is None:\n",
    "                self.episode_prob_actions = action_probs\n",
    "            else:\n",
    "                self.episode_prob_actions = tf.concat([self.episode_prob_actions, action_probs], axis=0)\n",
    "        \n",
    "        return  tf.squeeze(action, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer= tf.keras.optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the update policy function\n",
    "This function is called after we gather a complete episode, and it does the following tasks:\n",
    "1. Calculate the discounted Rewards: $\\hat{Q}(s,a)=\\sum_{i=0} \\gamma^i r_i$\n",
    "2. Calculate the baseline (rewards mean) and normalize the discounted rewards\n",
    "3. Calculate the entropy loss (Optional)\n",
    "4. Calculate the Policy loss\n",
    "5. Calculate the complete loss\n",
    "6. Do the loss backpropagation (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def update_policy(use_baseline = False, use_entropy = False):\n",
    "    R = 0\n",
    "    rewards = []\n",
    "\n",
    "    # Discount future rewards starting from the most recent reward backwards\n",
    "    for r in reversed(policy.episode_rewards):\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Scale rewards to make the advantage be 1/-1 (Zero-mean Unit variance)\n",
    "    # This is one of multiple ways to implement the advantage, for example if you use another network\n",
    "    # to learn the advantage this will become the actor-critic algorithm.\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float64)\n",
    "    \n",
    "    if use_baseline:\n",
    "        baseline = tf.math.reduce_mean(rewards)\n",
    "        # Calculate the value estimation with a baseline (Without the baseline will be harder to converge)\n",
    "        Q_estim = (rewards - baseline) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    else:\n",
    "        Q_estim = rewards\n",
    "\n",
    "    # Calculate the entropy \n",
    "    entropy_loss = -entropy_beta * (tf.math.reduce_sum(\n",
    "        policy.episode_prob_actions * policy.episode_log_prob_actions_dist))\n",
    "    \n",
    "    # Calculate the policy loss    \n",
    "    police_loss = tf.math.reduce_sum(-Q_estim * policy.episode_log_prob_actions)\n",
    "    \n",
    "    if use_entropy:\n",
    "        # Calculate loss log_prob(action) * rewards\n",
    "        loss = police_loss - entropy_loss    \n",
    "    else:\n",
    "        loss = police_loss\n",
    "    \n",
    "    # Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss)\n",
    "    policy.reward_history.append(np.sum(policy.episode_rewards))\n",
    "    policy.reset()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check untrained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for time in range(50):\n",
    "    # Get an action from the Policy\n",
    "    action = policy(state)\n",
    "\n",
    "    # Render screen\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage length (last 100 episodes): 9.00 Average Rewards 1.00\n",
      "Episode 50\tAverage length (last 100 episodes): 24.67 Average Rewards 1.00\n",
      "Episode 100\tAverage length (last 100 episodes): 27.07 Average Rewards 1.00\n",
      "Episode 150\tAverage length (last 100 episodes): 29.50 Average Rewards 1.00\n",
      "Episode 200\tAverage length (last 100 episodes): 30.11 Average Rewards 1.00\n",
      "Episode 250\tAverage length (last 100 episodes): 31.37 Average Rewards 1.00\n",
      "Episode 300\tAverage length (last 100 episodes): 29.87 Average Rewards 1.00\n",
      "Episode 350\tAverage length (last 100 episodes): 28.80 Average Rewards 1.00\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Record operations for automatic-differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        for time in range(1000):\n",
    "            # Get an action from the Policy and keep the historical data\n",
    "            action = policy(state, keep_history=True)\n",
    "\n",
    "            # Step through environment using chosen action\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Save reward\n",
    "            policy.episode_rewards.append(reward)\n",
    "            rewards_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Calculate score to determine when the environment has been solved\n",
    "        length_episode.append(time)\n",
    "        mean_length_episode = np.mean(length_episode[-100:])\n",
    "        mean_rewards_episode = np.mean(rewards_episode)\n",
    "        \n",
    "        # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "        loss = update_policy()\n",
    "        \n",
    "    # Calculate the gradient of the loss with respect to the model parameters\n",
    "    gradients = tape.gradient(loss, policy.trainable_variables)\n",
    "    # Run optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, policy.trainable_variables))\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for time in range(1000):\n",
    "    action = policy(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving sample efficiency\n",
    "Policy Gradient algorithms are already sample-efficiency worse than value based algorithms. Calculating a baseline and subtracting from your value estimate $\\hat{Q}$ will improve sample efficiency. Here is a list of poossible choices for baselines:\n",
    "* Some constant value, for example the mean of the discounted rewards\n",
    "* Moving average of discounted rewards\n",
    "* Value of state V(s)\n",
    "Also depending if we use anohter network to learn out baseline we will be working on the Actor-Critic family o algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(1000):\n",
    "        # Get an action from the Policy and keep the historical data\n",
    "        action = policy(state, keep_history=True)\n",
    "\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Save reward\n",
    "        policy.episode_rewards.append(reward)\n",
    "        rewards_episode.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate score to determine when the environment has been solved\n",
    "    length_episode.append(time)\n",
    "    mean_length_episode = np.mean(length_episode[-100:])\n",
    "    mean_rewards_episode = np.mean(rewards_episode)\n",
    "    \n",
    "    # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "    update_policy(use_baseline=True)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Exploration/Exploitation\n",
    "In order to avoid the policy to conferge fast and stop exploring (Get stuck in local minima) in the environment, there is a trick called entropy bonus.\n",
    "$$H(\\pi)=-\\sum{\\pi_{\\theta}(a|s).log\\pi_{\\theta}(a|s)}$$\n",
    "This will simply show how much uncertain the policy is about it's actions. \n",
    "We just need to subtract the entropy bons from the loss function, forcing the policy to take more time to be certain and exploring more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(1000):\n",
    "        # Get an action from the Policy and keep the historical data\n",
    "        action = policy(state, keep_history=True)\n",
    "\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Save reward\n",
    "        policy.episode_rewards.append(reward)\n",
    "        rewards_episode.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate score to determine when the environment has been solved\n",
    "    length_episode.append(time)\n",
    "    mean_length_episode = np.mean(length_episode[-100:])\n",
    "    mean_rewards_episode = np.mean(rewards_episode)\n",
    "    \n",
    "    # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "    update_policy(use_baseline=True, use_entropy=True)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = policy(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
