{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "### Introduction\n",
    "On this notebook we will train a policy network to solve a particular problem on the OpenAI Environment. We will train this policy network with the REINFORCE algorithm.\n",
    "![alt text](imgs/policy_stochastic.png \"Game\")\n",
    "\n",
    "#### REINFORCE Algorithm\n",
    "#### Short Introduction\n",
    "The REINFORCE algorithm is one implementation of the Policy Gradient family of algorithms, the idea of the algorithm is to have the policy represented by a neural network with $\\theta$ parameters. The main idea of the algorithm is to change the network parameters to make the actions that provided positive rewards to be more probable to happen. The training phase optimize the following loss function:\n",
    "$$\\mathcal{L}=-\\hat{Q}(s,a).log\\pi_{\\theta}(a|s)$$\n",
    "Where:\n",
    "* $\\hat{Q}(s,a)$: It's an estimate of the state action function, that will modulate the probability of good actions to happen more often.\n",
    "* $\\pi_{\\theta}(a|s)$: It's the neural network that represent the policy and returns an distribution of actions given an state.\n",
    "\n",
    "The steps of the algorithm\n",
    "1. Initialize Network at random\n",
    "2. Play N episodes saving their transitions \"k\" (s,a,r,s')\n",
    "3. For every episode calculate $\\hat{Q}(s,a)=\\sum_{i=0} \\gamma^i r_i$\n",
    "4. Perform SGD to minimize the loss: $\\mathcal{L}=-\\hat{Q}(s,a).log\\pi_{\\theta}(a|s)$\n",
    "5. Repeat 2 until convergence \n",
    "\n",
    "#### Disadvantages of Policy Gradients\n",
    "* Full episodes are required, we need to wait a full episode to complete\n",
    "* High Gradients Variance: This issue can be handled by subtracting a baseline from the Value estimation\n",
    "* Exploration: The agent can converge to a local-optimal area and won't explore efficiently anymore. This can be solved by the Entropy Bonus technique that basically subtract the entropy of the policy from the loss function.\n",
    "* Correlation between samples: This can be remedy by using parallel environments with same policy and using the experiences from different environments to train the policy.\n",
    "* Less sample efficient: To deal with this we need another algorithm (Actor-Critic)\n",
    "\n",
    "The image bellow can highlight the full episodic issue, where we have 2 trajectories where one or more actions could be bad, but as the final total score is good those bad actions will be averaged. That's one of the reasons why Policy Gradient methods are less sample efficient.\n",
    "![alt text](imgs/episode_problem.png \"Game\")\n",
    "\n",
    "#### Cartpole Rules\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "#### References\n",
    "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
    "* http://karpathy.github.io/2016/05/31/rl/\n",
    "* https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf\n",
    "* https://leimao.github.io/article/REINFORCE-Policy-Gradient/\n",
    "* https://towardsdatascience.com/an-intuitive-explanation-of-policy-gradient-part-1-reinforce-aa4392cbfd3c\n",
    "* https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f\n",
    "* https://gist.github.com/tamlyn/a9d2b3990f9dab0f82d1dfc1588c876a\n",
    "* http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "* https://www.youtube.com/watch?v=iOh7QUZGyiU&list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs\n",
    "* https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html\n",
    "* https://medium.com/@gabogarza/deep-reinforcement-learning-policy-gradients-8f6df70404e6\n",
    "* https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4\n",
    "* http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf\n",
    "* https://fosterelli.co/entropy-loss-for-reinforcement-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment and Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Amount of rewards consider as win: 200\n",
      "Action space: 4\n",
      "Observation space: (8,)\n",
      "Reward range: (-inf, inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardo_a/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "# Uncomment for this environment (takes longer to train)\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "entropy_beta = 0.01\n",
    "gamma = 0.99\n",
    "num_episodes=1500\n",
    "\n",
    "# Show some information from the environment\n",
    "print('Amount of rewards consider as win:', env.spec.reward_threshold)\n",
    "print('Action space:', env.action_space.n)\n",
    "print('Observation space:', env.observation_space.shape)\n",
    "print('Reward range:', env.reward_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Random Seeds\n",
    "To make the experiment reproducible, we will fix the following random seeds:\n",
    "* OpenAI Gym\n",
    "* Numpy\n",
    "* Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network\n",
    "Define the policy Neural Network, which inputs will be the state and output an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(Model):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        state_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        num_hidden = 128\n",
    "\n",
    "        self.FC1 = Dense(num_hidden, activation='relu', input_dim=state_space, use_bias=False)\n",
    "        self.DP1 = Dropout(rate=0.5)\n",
    "        self.FC2 = Dense(action_space, activation='softmax', use_bias=False)\n",
    "\n",
    "        # Overall reward and loss history (Just for plotting training information)\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Episode policy log probability(action) and reward history\n",
    "        self.episode_log_prob_actions = []\n",
    "        self.episode_log_prob_actions_dist = []\n",
    "        self.episode_prob_actions = []\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def call(self, x, keep_history=False):\n",
    "        # Convert state to tensor\n",
    "        x = self.FC1(x)\n",
    "        x = self.DP1(x)\n",
    "        action_probs = self.FC2(x)\n",
    "        \n",
    "        # Execute model and sample it's outputs\n",
    "        action = tf.random.categorical(action_probs,1)\n",
    "        \n",
    "        #print('action_probs:', action_probs.size())\n",
    "        #print('distribution.log_prob:', distribution.log_prob(action).size())\n",
    "        #print('F.log_prob(action_probs):', torch.log(action_probs).size())\n",
    "        \n",
    "        # Concatenate log probability of the action to be used on the loss function\n",
    "        if keep_history:\n",
    "            self.episode_log_prob_actions = tf.concat([self.episode_log_prob_actions, \n",
    "                                                     distribution.log_prob(action).reshape(1)])\n",
    "            self.episode_log_prob_actions_dist = tf.concat([self.episode_log_prob_actions_dist, \n",
    "                                                     torch.log(action_probs)])\n",
    "            self.episode_prob_actions = tf.concat([self.episode_prob_actions, action_probs])\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer= tf.keras.optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the update policy function\n",
    "This function is called after we gather a complete episode, and it does the following tasks:\n",
    "1. Calculate the discounted Rewards: $\\hat{Q}(s,a)=\\sum_{i=0} \\gamma^i r_i$\n",
    "2. Calculate the baseline (rewards mean) and normalize the discounted rewards\n",
    "3. Calculate the entropy loss (Optional)\n",
    "4. Calculate the Policy loss\n",
    "5. Calculate the complete loss\n",
    "6. Do the loss backpropagation (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(use_baseline = False, use_entropy = False):\n",
    "    R = 0\n",
    "    rewards = []\n",
    "\n",
    "    # Discount future rewards starting from the most recent reward backwards\n",
    "    for r in reversed(policy.episode_rewards):\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Scale rewards to make the advantage be 1/-1 (Zero-mean Unit variance)\n",
    "    # This is one of multiple ways to implement the advantage, for example if you use another network\n",
    "    # to learn the advantage this will become the actor-critic algorithm.\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    \n",
    "    if use_baseline:\n",
    "        baseline = tf.math.reduce_mean(rewards)\n",
    "        # Calculate the value estimation with a baseline (Without the baseline will be harder to converge)\n",
    "        Q_estim = (rewards - baseline) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    else:\n",
    "        Q_estim = rewards\n",
    "\n",
    "    # Calculate the entropy \n",
    "    entropy_loss = -entropy_beta * (tf.math.reduce_sum(policy.episode_prob_actions * policy.episode_log_prob_actions_dist))\n",
    "    \n",
    "    # Calculate the policy loss\n",
    "    police_loss = tf.math.reduce_sum(-Q_estim * policy.episode_log_prob_actions)\n",
    "    \n",
    "    if use_entropy:\n",
    "        # Calculate loss log_prob(action) * rewards\n",
    "        loss = police_loss - entropy_loss    \n",
    "    else:\n",
    "        loss = police_loss\n",
    "\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.episode_rewards))\n",
    "    policy.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check untrained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer dense_12 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-75166ce2458b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Get an action from the Policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Render screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-01fee750c4e7>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, keep_history)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Convert state to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDP1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m     input_spec.assert_input_compatibility(\n\u001b[0;32m-> 1701\u001b[0;31m         self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   1702\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m                          \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                          \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer dense_12 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [8]"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for time in range(50):\n",
    "    # Get an action from the Policy\n",
    "    action = policy(state)\n",
    "\n",
    "    # Render screen\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(1000):\n",
    "        # Get an action from the Policy and keep the historical data\n",
    "        action = policy(state, keep_history=True)\n",
    "\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Save reward\n",
    "        policy.episode_rewards.append(reward)\n",
    "        rewards_episode.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate score to determine when the environment has been solved\n",
    "    length_episode.append(time)\n",
    "    mean_length_episode = np.mean(length_episode[-100:])\n",
    "    mean_rewards_episode = np.mean(rewards_episode)\n",
    "    \n",
    "    # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "    update_policy()\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for time in range(1000):\n",
    "    action = policy(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving sample efficiency\n",
    "Policy Gradient algorithms are already sample-efficiency worse than value based algorithms. Calculating a baseline and subtracting from your value estimate $\\hat{Q}$ will improve sample efficiency. Here is a list of poossible choices for baselines:\n",
    "* Some constant value, for example the mean of the discounted rewards\n",
    "* Moving average of discounted rewards\n",
    "* Value of state V(s)\n",
    "Also depending if we use anohter network to learn out baseline we will be working on the Actor-Critic family o algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(1000):\n",
    "        # Get an action from the Policy and keep the historical data\n",
    "        action = policy(state, keep_history=True)\n",
    "\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Save reward\n",
    "        policy.episode_rewards.append(reward)\n",
    "        rewards_episode.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate score to determine when the environment has been solved\n",
    "    length_episode.append(time)\n",
    "    mean_length_episode = np.mean(length_episode[-100:])\n",
    "    mean_rewards_episode = np.mean(rewards_episode)\n",
    "    \n",
    "    # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "    update_policy(use_baseline=True)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Exploration/Exploitation\n",
    "In order to avoid the policy to conferge fast and stop exploring (Get stuck in local minima) in the environment, there is a trick called entropy bonus.\n",
    "$$H(\\pi)=-\\sum{\\pi_{\\theta}(a|s).log\\pi_{\\theta}(a|s)}$$\n",
    "This will simply show how much uncertain the policy is about it's actions. \n",
    "We just need to subtract the entropy bons from the loss function, forcing the policy to take more time to be certain and exploring more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "length_episode = []\n",
    "rewards_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and record the starting state\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(1000):\n",
    "        # Get an action from the Policy and keep the historical data\n",
    "        action = policy(state, keep_history=True)\n",
    "\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Save reward\n",
    "        policy.episode_rewards.append(reward)\n",
    "        rewards_episode.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate score to determine when the environment has been solved\n",
    "    length_episode.append(time)\n",
    "    mean_length_episode = np.mean(length_episode[-100:])\n",
    "    mean_rewards_episode = np.mean(rewards_episode)\n",
    "    \n",
    "    # Calculate the discounted rewards on the episode, calculate loss and backpropagate\n",
    "    update_policy(use_baseline=True, use_entropy=True)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tAverage length (last 100 episodes): {:.2f} Average Rewards {:.2f}'.format(\n",
    "            episode, mean_length_episode, mean_rewards_episode))\n",
    "\n",
    "    if mean_length_episode > env.spec.reward_threshold:\n",
    "        print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "              .format(episode, mean_length_episode, time))\n",
    "        # Stop training after solving the environment\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "# Create grid 2x1 plots\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "\n",
    "# Calculate the mean of the rewards over time\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "# Calculate the standard deviation over time\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "\n",
    "# Plot graph 1\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "# Plot graph 2\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = policy(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7, shape=(), dtype=int32, numpy=6>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_sum([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
